{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "tXnKo5fe6SqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook consists of 5 parts.  The point-value of each part is indicated in the section header.\n",
        "\n",
        "Some executable cells have code, while other executable cells have a comment 'Challenge Cell' and a point value.\n",
        "\n",
        "The executable cells with code are not worth any points, but must be executed to successfully complete and execute the challenge cells."
      ],
      "metadata": {
        "id": "ech-M8uT6XTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "pwZPWD_tQTd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Pre-Midterm (Modules 01-06)\n",
        "Total Part Value: 10 Points"
      ],
      "metadata": {
        "id": "FTEBjEzL4tQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge: NLP Pipeline (8 Points)"
      ],
      "metadata": {
        "id": "ycluu6DX0Eq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The twenty newsgroups dataset will be used for this lab. It is already loaded in the sklearn library."
      ],
      "metadata": {
        "id": "MYsoQeDD2iNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the necessary data\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "count_vect = CountVectorizer()\n",
        "print(\"twenty_train ready\")"
      ],
      "metadata": {
        "id": "7sCg3pdr2bYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Data (2.0 Points)\n",
        "# Fit the twenty_train.data to CountVectorizer and store it in a variable X_train_counts, and print the shape\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "print(X_train_counts.shape)"
      ],
      "metadata": {
        "id": "bSc-jCJ82y6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us obtain the tfidf features from the text."
      ],
      "metadata": {
        "id": "sHvyW2w33PpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TF-IDF (2.0 Points)\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "# Fit the data to TfidfTransformer and store it in a variable X_train_tfidf, and print the shape\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "print(X_train_tfidf.shape)"
      ],
      "metadata": {
        "id": "z6li5DlX3S-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will use the Naive Bayes classifier to classify text. It is also available in the sklearn library."
      ],
      "metadata": {
        "id": "qsGbCSkf3b_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
        "clf"
      ],
      "metadata": {
        "id": "nH85DRB73bNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pipeline function is available in the Sklearn library. It takes the different steps of the pipeline one by one:\n",
        "\n",
        "- ('vect', CountVectorizer()),\n",
        "- ('tfidf', TfidfTransformer()),\n",
        "- ('clf', MultinomialNB())"
      ],
      "metadata": {
        "id": "Ow2MZJjj3r5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pipeline for MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', MultinomialNB())])\n",
        "text_clf"
      ],
      "metadata": {
        "id": "x9eb8nMu3s-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Pipeline\n",
        "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
        "text_clf"
      ],
      "metadata": {
        "id": "MzLoyG9v30ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Bayes Predictions\n",
        "import numpy as np\n",
        "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "predicted = text_clf.predict(twenty_test.data)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "metadata": {
        "id": "fsAfIhfy36-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given the following chronological steps of the pipeline:\n",
        "\n",
        "- ('vect', CountVectorizer())\n",
        "- ('tfidf', TfidfTransformer())\n",
        "- ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42))\n",
        "\n",
        "Now, your task is to design the <b>pipeline</b>"
      ],
      "metadata": {
        "id": "YXMRk8e93_mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Pipeline for SVM using SGD (2.0 Points)\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "# Similar to 'Create Pipeline for MultinomialNB', design the pipeline using SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42)\n",
        "# and store it in a variable text_clf_svm\n",
        "#text_clf_svm = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42)\n",
        "\n",
        "text_clf_svm = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(\n",
        "        loss='hinge',\n",
        "        penalty='l2',\n",
        "        alpha=1e-3,\n",
        "        max_iter=5,\n",
        "        random_state=42\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "uPl_UXgA4Fu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate SVM Predictions (1.0 Points)\n",
        "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
        "# Similar to 'Bayes Predictions' (above),  predict the labels for twenty_test data\n",
        "# Specifically, use the function text_clf_svm.predict, the test data can be referenced by twenty_test.data\n",
        "\n",
        "# prediction accuracy\n",
        "text_clf_svm = text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
        "\n",
        "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
        "\n",
        "np.mean(predicted_svm == twenty_test.target)"
      ],
      "metadata": {
        "id": "sJEdtapo4nCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Challenge: Define a sequential model"
      ],
      "metadata": {
        "id": "an2ayob758-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Keras libraries and packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import plot_model\n",
        "from keras import Sequential"
      ],
      "metadata": {
        "id": "ArruvrNq6JpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model you need to make (2 Points)\n",
        "# fill in the parameters so that you can create the model above\n",
        "model = Sequential() # initialize sequential model\n",
        "model.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'tanh', input_dim = 15)) # Dense layer with 10 Neurons\n",
        "model.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'tanh')) # Dense layer with 32 Neurons\n",
        "model.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'tanh')) # Dense layer with 32 Neurons\n",
        "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) # Dense output layer with 1 neuron, sigmoid activation\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "gbFtx4EZ6ThB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Recurrent Neural Network (Modules 08)\n",
        "Total Part Value: 10 Points"
      ],
      "metadata": {
        "id": "tKMPJuhf46Yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge: Bag of Words\n"
      ],
      "metadata": {
        "id": "ECtGKPJPwmfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the vectors for the following sentences using Bag of Words approach:\n",
        "*   Here we go again\n",
        "*   Go and play baseball\n",
        "*   Baseball and tennis are popular here"
      ],
      "metadata": {
        "id": "x3xwQYmk8wKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the data and helper functions\n",
        "\n",
        "docs = [\n",
        "    'Here we go again',\n",
        "    'Go and play baseball',\n",
        "    'Baseball and tennis are popular here'\n",
        "]\n",
        "tokenized_docs = [sentence.split() for sentence in docs]\n",
        "vocab = [word.lower() for sentence in tokenized_docs for word in sentence]\n",
        "def vectorize(sentence):\n",
        "  vectorized = [0] * len(vocab)\n",
        "\n",
        "  i = 0\n",
        "  for token in sentence.split():\n",
        "    vectorized[vocab.index(token.lower())] += 1\n",
        "    i += 1\n",
        "  return vectorized"
      ],
      "metadata": {
        "id": "kTis8VVK82RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the sentences (2.0 Points)\n",
        "vectorized_list = []\n",
        "for sentence in docs:\n",
        "  vectorized = vectorize(sentence)\n",
        "  vectorized_list.append(vectorized)\n",
        "\n",
        "vectorized_list"
      ],
      "metadata": {
        "id": "U6FEpjfI9b6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Designing Neural Networks\n"
      ],
      "metadata": {
        "id": "McxqZs7uAKE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have given you the image of a model and its layers. The code is partially made for you. Your task is to fill in the blanks in the code block to create an exact same model."
      ],
      "metadata": {
        "id": "bYmVUjewAQhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import SimpleRNN, Dense, LSTM\n",
        "from keras.layers import Bidirectional"
      ],
      "metadata": {
        "id": "Vg-yznQgAZpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the Neural Network Model (4.0 Points)\n",
        "# The model you need to make\n",
        "model = Sequential() # initialize sequential model\n",
        "model.add(LSTM(126, input_shape=(70,1), return_sequences=True)) # LSTM layer with 126 neurons\n",
        "model.add(LSTM(63, return_sequences=True)) # LSTM layer with 63 neurons\n",
        "model.add(LSTM(63)) # LSTM layer with 63 neurons\n",
        "model.add(Dense(26,activation='relu')) # Dense layer with 26 neurons\n",
        "model.add(Dense(18,activation='relu')) # Dense layer with 18 neurons, relu activation\n",
        "model.add(Dense(1,activation='relu')) # Dense output layer with 1 neuron, relu activation\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "yVQ4556YAUGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional RNN\n",
        "- Design a sequential model that takes an input vector of shape (10,20)\n",
        "- Add a bidirectional LSTM layer of 25 neurons\n",
        "- Add another bidirectional LSTM layer of 15 neurons\n",
        "- Add a dense layer of 10 neurons"
      ],
      "metadata": {
        "id": "CPCX_OuRAnoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the Neural Network Model (4.0 Points)\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(25, return_sequences=True), input_shape=(10,20)))\n",
        "model.add(Bidirectional(LSTM(15)))\n",
        "model.add(Dense(10))\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "TYjjzLIdArmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Model Deployment (Module 09)\n",
        "Total Part Value: 5 Points"
      ],
      "metadata": {
        "id": "pLlzYiEx8zFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simulating Streamlit\n",
        "In Module09, we used Streamlit to select a classification algorithm.\n",
        "Your task is to use the code snippets below to implement the same functionality in this Jupyter Notebook, but using input provided by the 'input()' method instead of the Streamlit User Interface elements."
      ],
      "metadata": {
        "id": "kTYxlqrJ_G3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "  # Code Snippet #1\n",
        "  trainData = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "  print(\"SVM selected\")\n",
        "  classificationPipeline = Pipeline([('bow', CountVectorizer()), ('vector', TfidfTransformer()), ('classifier', SGDClassifier(loss='hinge', penalty='l1', alpha=0.0005, l1_ratio=0.17))])\n",
        "  #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "  classificationPipeline = classificationPipeline.fit(trainData.data, trainData.target)\n",
        "  test_set = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "  dataPrediction = classificationPipeline.predict(test_set.data)\n",
        "  print(\"SVM:\")    \n",
        "  print(np.mean(dataPrediction == test_set.target))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DYLVgxFv_j7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "  # Code Snippet #2\n",
        "  trainData = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "  print(\"Naive Bayes selected\")\n",
        "  classificationPipeline = Pipeline([('bow', CountVectorizer()), ('vector', TfidfTransformer()), ('classifier', MultinomialNB())])\n",
        "  classificationPipeline = classificationPipeline.fit(trainData.data, trainData.target)\n",
        "  test_set = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "  dataPrediction = classificationPipeline.predict(test_set.data)\n",
        "  print(\"Accuracy of Naive Bayes:\")\n",
        "  print(np.mean(dataPrediction == test_set.target))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZyVT9BwQ_n5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the user's desired classification method\n",
        "cs = [\"Naive Bayes\",\"SVM\"]\n",
        "print(\"Pick a classification method:\")\n",
        "for option in cs:\n",
        "  print(f\"\\t {option}\")\n",
        "\n",
        "classification_space = input()"
      ],
      "metadata": {
        "id": "fNSOq0kycvBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the correct Code Snippet based on the user's choice (5.0 Points)\n",
        "if classification_space == \"Naive Bayes\":\n",
        "  trainData = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "  print(\"Naive Bayes selected\")\n",
        "  classificationPipeline = Pipeline([('bow', CountVectorizer()), ('vector', TfidfTransformer()), ('classifier', MultinomialNB())])\n",
        "  classificationPipeline = classificationPipeline.fit(trainData.data, trainData.target)\n",
        "  test_set = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "  dataPrediction = classificationPipeline.predict(test_set.data)\n",
        "  print(\"Accuracy of Naive Bayes:\")\n",
        "  print(np.mean(dataPrediction == test_set.target))\n",
        "\n",
        "if classification_space == \"SVM\":\n",
        "  trainData = fetch_20newsgroups(subset='train', shuffle=True)\n",
        "  print(\"SVM selected\")\n",
        "  classificationPipeline = Pipeline([('bow', CountVectorizer()), ('vector', TfidfTransformer()), ('classifier', SGDClassifier(loss='hinge', penalty='l1', alpha=0.0005, l1_ratio=0.17))])\n",
        "  #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "  classificationPipeline = classificationPipeline.fit(trainData.data, trainData.target)\n",
        "  test_set = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "  dataPrediction = classificationPipeline.predict(test_set.data)\n",
        "  print(\"SVM:\")\n",
        "  print(np.mean(dataPrediction == test_set.target))\n"
      ],
      "metadata": {
        "id": "swhBsIax5dQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Chatbots (Module 10)\n",
        "Total Part Value: 5 Points"
      ],
      "metadata": {
        "id": "nM-u3o-uBqXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Customize a Chatbot"
      ],
      "metadata": {
        "id": "RhWhu7RdCFDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a chatbot\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string # to process standard python strings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "f=!wget https://ethaneldridge.github.io/cisd410/M16-FinalExam/chatbots.txt\n",
        "f=open('chatbots.txt','r',errors = 'ignore')\n",
        "\n",
        "raw=f.read()\n",
        "raw=raw.lower() # converts to lowercase\n",
        "nltk.download('punkt') # first-time use only\n",
        "nltk.download('wordnet') # first-time use only\n",
        "nltk.download('punkt_tab') # Download punkt_tab resource\n",
        "sent_tokens = nltk.sent_tokenize(raw) # converts to list of sentences\n",
        "word_tokens = nltk.word_tokenize(raw) # converts to list of words\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "# WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# This removes the punctuation from sentences\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)\n",
        "\n",
        "def response(user_request):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_request)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, token_pattern=None, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "metadata": {
        "id": "TDDgI4OXF5LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the chatbot\n",
        "flag=True\n",
        "print(\"Hi. I will answer your queries about Chatbots. To exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_request = input()\n",
        "    user_request=user_request.lower()\n",
        "    if(user_request!='bye'):\n",
        "        if(user_request=='thanks' or user_request=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_request)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_request))\n",
        "            else:\n",
        "                print(\"ROBO: \",end=\"\")\n",
        "                print(response(user_request))\n",
        "                sent_tokens.remove(user_request)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! \")"
      ],
      "metadata": {
        "id": "tIstG_guHUa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have successfully built our first chatbot. Your challenge is to now change this chatbot. For our example, we used the Wikipedia page for chatbots as our corpus. Now use the information from this page: https://www.chatcompose.com/what-are-chatbots.html as the chatbot corpus and retrain your chatbot."
      ],
      "metadata": {
        "id": "vMp5HoyBHCfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Customize the chatbot (5.0 Points)\n",
        "f=!wget https://www.chatcompose.com/what-are-chatbots.html\n",
        "f=open('chatbots.txt','r',errors = 'ignore')\n",
        "\n",
        "raw=f.read()\n",
        "raw=raw.lower() # converts to lowercase\n",
        "nltk.download('punkt') # first-time use only\n",
        "nltk.download('wordnet') # first-time use only\n",
        "nltk.download('punkt_tab') # Download punkt_tab resource\n",
        "sent_tokens = nltk.sent_tokenize(raw) # converts to list of sentences\n",
        "word_tokens = nltk.word_tokenize(raw) # converts to list of words\n",
        "\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "# WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# This removes the punctuation from sentences\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"Howdy, what's up?\", \"Hey, how may I be of assistance?\", \"Hi, how's it going?\", \"Hello, how can I help you?\", \"I am glad to be chatting!\"]\n",
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)\n",
        "\n",
        "def response(user_request):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_request)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, token_pattern=None, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf<0):\n",
        "        robo_response=robo_response+\"I'm not sure I understand. Can you elaborate or rephrase that please.\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "\n",
        "flag=True\n",
        "print(\"Hi. I can answer your questions about Chatbots. To exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_request = input()\n",
        "    user_request=user_request.lower()\n",
        "    if(user_request!='bye'):\n",
        "        if(user_request=='thanks' or user_request=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_request)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_request))\n",
        "            else:\n",
        "                print(\"ROBO: \",end=\"\")\n",
        "                print(response(user_request))\n",
        "                sent_tokens.remove(user_request)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! \")"
      ],
      "metadata": {
        "id": "-Rxczo2MH2cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 5: Translation (Module 11)\n",
        "Total Part Value: 10 Points"
      ],
      "metadata": {
        "id": "T6qYaoJJIm4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Argos Translate to perform Language Translation"
      ],
      "metadata": {
        "id": "skkZ8L5ZJhk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the library\n",
        "!pip install argostranslate\n"
      ],
      "metadata": {
        "id": "l1aUPvMvJg9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries and create the helper functions\n",
        "import argostranslate.package\n",
        "import argostranslate.translate\n",
        "argostranslate.package.update_package_index()\n",
        "available_packages = argostranslate.package.get_available_packages()\n",
        "\n",
        "for package in available_packages:\n",
        "  if package.from_code == \"en\":\n",
        "    print(f\"Package {package} package.from_code {package.from_code}, package.to_code {package.to_code}, package.code {package.code}\")\n",
        "\n",
        "def translate(text, target_language):\n",
        "  from_code = \"en\"\n",
        "  package_to_install = next(\n",
        "    filter(\n",
        "      lambda x: x.from_code == from_code and x.to_code == target_language, available_packages\n",
        "    )\n",
        "  )\n",
        "  argostranslate.package.install_from_path(package_to_install.download())\n",
        "  return argostranslate.translate.translate(text, from_code, target_language)"
      ],
      "metadata": {
        "id": "1iUEY84dadaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate the sentence 'A robot may not injure a human being or, through inaction, allow a human being to come to harm.' into Spanish (3.0 Points)\n",
        "sentence = 'A robot may not injure a human being or, through inaction, allow a human being to come to harm.'\n",
        "translate(sentence, 'es')"
      ],
      "metadata": {
        "id": "M-wZHbUEKu93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate a sentence of your choice into a language of your choice (2.0 Points)\n",
        "personal_sentence = \"Hello, my name is Oliver and I am a high school student who is super interested in AI and Computer Science. I am so happy to meet you.\"\n",
        "translate(personal_sentence, 'az')"
      ],
      "metadata": {
        "id": "-xOoR8jJK6Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using MT5"
      ],
      "metadata": {
        "id": "yJy1jZehRelG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# process the article_text\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
        "\n",
        "article_text = \"\"\"Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.\"\"\"\n",
        "\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "input_ids = tokenizer(\n",
        "    [WHITESPACE_HANDLER(article_text)],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")[\"input_ids\"]\n",
        "\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=84,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4\n",
        ")[0]\n",
        "\n",
        "summary = tokenizer.decode(\n",
        "    output_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "zoUtu7UHRi2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question01: What NLP Task is performed by the MT5 LLM? (1 Point)\n",
        "\"The LLM is summarizing and condensing information\""
      ],
      "metadata": {
        "id": "LVCLGEAITAbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the MT5 LLM code so that it will instead operate on this text:(4 Points)\n",
        "# (from Andrew Ng https://aifund.ai/insights-written-statement-of-andrew-ng-before-the-u-s-senate-ai-insight-forum/):\n",
        "article_text = \"\"\"\n",
        "AI technology is used in applications in healthcare, underwriting, self-driving, social media, and other sectors. With some applications, there are risks of significant harm. We want:\n",
        "\n",
        "Medical devices to be safe\n",
        "Underwriting software to be fair, and not discriminate based on protected characteristics\n",
        "Self-driving cars to be safe\n",
        "Social media to be governed in a way that respects freedom of speech but also does not subject us to foreign actors’ disinformation campaigns\n",
        "When we think about specific AI applications, we can figure out what outcomes we do want (such as improved healthcare) and do not want (such as medical products that make false claims) and regulate accordingly.\n",
        "\n",
        "A fundamental distinction in decisions about how to regulate AI is between applications vs. technology.\n",
        "\n",
        "Nikola Tesla’s invention of the AC (alternating current) electric motor was a technology. When this technology is incorporated into either a blender or an electric car, the blender or car is an application. Electric motors are useful for so many things it is hard to effectively regulate them separately from thinking about concrete use cases. But when we look at blenders and electric cars, we can systematically identify benefits and risks and work to enable the benefits while limiting risks.\n",
        "\n",
        "Whereas motors help us with physical work, AI helps us with intellectual work.\n",
        "\n",
        "In the case of AI, engineers and scientists will typically write software and have it learn from a lot of data. This AI system may live in a company’s datacenter and be subject to testing and experimentation, but it is not yet made available to any end-user. This is AI technology — essentially a piece of math that can be used for many different applications. When engineers then use this technology to build a piece of software for a particular purpose, such as medical diagnosis, it then becomes an application.\n",
        "\"\"\"\n",
        "# process the article_text\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
        "\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "input_ids = tokenizer(\n",
        "    [WHITESPACE_HANDLER(article_text)],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=512\n",
        ")[\"input_ids\"]\n",
        "\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=84,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4\n",
        ")[0]\n",
        "\n",
        "summary = tokenizer.decode(\n",
        "    output_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "2yJPinDxTMRZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tXnKo5fe6SqY",
        "FTEBjEzL4tQl",
        "d3_XQdLG9RSR",
        "tKMPJuhf46Yf",
        "ioKAHN3J8OV7",
        "pLlzYiEx8zFj",
        "oc-JgPh6-PwJ",
        "4jTxUSh2-3GO",
        "bKrwfaH5_TCd",
        "KhOnMzTPCQQC",
        "ByONhofiBdrz",
        "pwVPVakjLkla",
        "auezmf_OMa88",
        "oWwZyNFeNuYc",
        "ILUwCxZeOnpL",
        "HcxkdLMaDFjD",
        "pvUPQDHiEE3P",
        "7HiOtNGpEQq9",
        "giSCQd7GDXWQ",
        "-eHAjkMIsNF2",
        "xCmUvQlcwpwM",
        "uyM2VAPowx0p",
        "S7sChRCNw8qn",
        "82wFZAFGxgee",
        "nrPv3qyvx9oj",
        "41ttJ2Xv2k6j",
        "8ZWDMgmJdD4l",
        "VF7LGixTdD4p"
      ],
      "gpuType": "T4",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}